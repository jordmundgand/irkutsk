{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFTQDDWKYdFf"
      },
      "outputs": [],
      "source": [
        "!wget https://lodmedia.hb.bizmrg.com/case_files/798783/train_dataset_train.zip\n",
        "!wget https://lodmedia.hb.bizmrg.com/case_files/798783/test_dataset_test.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "2Xm-ASYGYpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn"
      ],
      "metadata": {
        "id": "7hh9b4lxY1PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils.video import VideoStream\n",
        "from imutils.video import FPS\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import dbscan\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, RidgeClassifier, LogisticRegressionCV,Ridge,QuantileRegressor,PassiveAggressiveClassifier\n",
        "from sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier,RandomForestClassifier,VotingClassifier,RandomForestRegressor,GradientBoostingClassifier,StackingRegressor,BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC,LinearSVR,SVR\n",
        "from sklearn.decomposition import TruncatedSVD,PCA,FactorAnalysis,IncrementalPCA,FastICA,KernelPCA,NMF\n",
        "from sklearn.preprocessing import RobustScaler,QuantileTransformer,PowerTransformer,PolynomialFeatures,KBinsDiscretizer,StandardScaler,OneHotEncoder,OrdinalEncoder,FunctionTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline,FeatureUnion,TransformerMixin\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor,LocalOutlierFactor\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedShuffleSplit,TimeSeriesSplit,GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer,SimpleImputer\n",
        "from sklearn.dummy import DummyRegressor,DummyClassifier\n",
        "from sklearn import set_config\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,roc_auc_score,accuracy_score,f1_score,classification_report\n",
        "import re\n",
        "from imblearn.pipeline import Pipeline as iPipeline\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import tensorflow as tf\n",
        "import pytesseract"
      ],
      "metadata": {
        "id": "lz6EJo8BYpQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/train_dataset_train.zip\n",
        "!unzip /content/test_dataset_test.zip"
      ],
      "metadata": {
        "id": "rVY9xSJeYpL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/Irkutsk/test.csv')\n",
        "train=pd.read_csv('/content/drive/MyDrive/Irkutsk/train.csv')\n",
        "train['date']=train['date'].astype('str').str.replace('2027','2022')\n",
        "train['date']=pd.to_datetime(train['date'],errors='coerce')"
      ],
      "metadata": {
        "id": "ld7qeKcAiDyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#парсим время в train1\n",
        "vs = cv2.VideoCapture('/content/train/train1.avi')\n",
        "slist,tlist=[],[]\n",
        "for cnt in range(0,13194):\n",
        "  vs.set(cv2.CAP_PROP_POS_MSEC,cnt*1000)\n",
        "  ret, frame = vs.read()\n",
        "  im=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[80:210,0:750]\n",
        "  im=cv2.resize(im, (16*750,16*130))\n",
        "  im=cv2.filter2D(im, -1, np.array([-1,5.5,-1]))\n",
        "  im=cv2.resize(im, (750//2,130//2))\n",
        "  slist.append(cnt)\n",
        "  tlist.append(pytesseract.image_to_string(im))\n",
        "train1_timing=pd.DataFrame({'Timing':slist,'Date_time':tlist})"
      ],
      "metadata": {
        "id": "g3b2_Y8zYpIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#парсим время в train2\n",
        "vs = cv2.VideoCapture('/content/train/train2.avi')\n",
        "slist,tlist=[],[]\n",
        "for cnt in range(0,20034):\n",
        "  vs.set(cv2.CAP_PROP_POS_MSEC,cnt*1000)\n",
        "  ret, frame = vs.read()\n",
        "  im=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[50:120,:560]\n",
        "  im=cv2.resize(im, (16*560,16*70))\n",
        "  im=cv2.filter2D(im, -1, np.array([-1,5.5,-1]))\n",
        "  im=cv2.resize(im, (560//2,70//2))\n",
        "  slist.append(cnt)\n",
        "  tlist.append(pytesseract.image_to_string(im))\n",
        "train2_timing=pd.DataFrame({'Timing':slist,'Date_time':tlist})"
      ],
      "metadata": {
        "id": "FQV3-Y2WYpEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#парсим время в test\n",
        "vs = cv2.VideoCapture('/content/train/test.avi')\n",
        "slist,tlist=[],[]\n",
        "for cnt in range(0,12459):\n",
        "  vs.set(cv2.CAP_PROP_POS_MSEC,cnt*1000)\n",
        "  ret, frame = vs.read()\n",
        "  im=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[50:120,:560]\n",
        "  im=cv2.resize(im, (16*560,16*70))\n",
        "  im=cv2.filter2D(im, -1, np.array([-1,5.5,-1]))\n",
        "  im=cv2.resize(im, (560//2,70//2))\n",
        "  slist.append(cnt)\n",
        "  tlist.append(pytesseract.image_to_string(im))\n",
        "test_timing=pd.DataFrame({'Timing':slist,'Date_time':tlist})"
      ],
      "metadata": {
        "id": "5yCRkXffbbVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#тайминг в трэйне\n",
        "train_timing=pd.concat((train1_timing,train2_timing))\n",
        "train_timing.index=range(len(train_timing))\n",
        "train_timing['ind']=pd.Series(train_timing.index)\n",
        "train_timing['hms1']=train_timing['Date_time'].str.extract(r\"([0-2]\\d:[0-5]\\d:[0-5]\\d)\")\n",
        "train_timing['hms2']=train_timing.loc[train_timing['hms1'].isna(),'Date_time'].str.extract(r\"(\\d:[0-5]\\d:[0-5]\\d)\")\n",
        "train_timing['hms']=(train_timing['hms1'].fillna('')+train_timing['hms2'].fillna(''))\n",
        "train_timing['d']=train_timing['Date_time'].str.extract(r\"(-2\\d-)\",expand=False).str.strip('-')\n",
        "train_timing['d']=pd.to_numeric(train_timing['d'],errors='coerce')\n",
        "dl=[train_timing['d'][0]]\n",
        "for cnt in train_timing['d'][1:]:\n",
        "  if cnt-dl[-1]==1:\n",
        "    dl.append(cnt)\n",
        "  else:\n",
        "    dl.append(dl[-1])\n",
        "train_timing['dy']=dl\n",
        "train_timing['h']=train_timing['hms'].str.split(':').str[0]\n",
        "train_timing['h']=pd.to_numeric(train_timing['h'],errors='coerce')\n",
        "hl=[train_timing['h'][0],train_timing['h'][0]]\n",
        "for cnt in zip(train_timing['h'][1:],train_timing['h'][1:].index):\n",
        "  print(cnt[0],hl[-1],min(cnt[0],hl[-1]+1))\n",
        "  if (cnt[0]>hl[-1] and cnt[0]-hl[-1]!=6) or hl[-1]-cnt[0]>9:\n",
        "    hl.append(min(cnt[0],hl[-2]+1))\n",
        "  else:\n",
        "    hl.append(hl[-1])\n",
        "train_timing['hr']=hl[1:]\n",
        "train_timing['Date_time4']=pd.to_datetime(\n",
        "    ('05-'+\n",
        "    (train_timing['dy']).astype('int').astype('str')+\n",
        "    '-2022 '+\n",
        "    (train_timing['hr']+100).astype('int').astype('str').str[1:]+\n",
        "    train_timing['hms'].str[-6:])\n",
        "    ,errors='coerce')\n",
        "train_timing['dh']=train_timing['Date_time4'].dt.day*24+train_timing['Date_time4'].dt.hour\n",
        "hl=[train_timing['dh'][0]]\n",
        "for cnt in train_timing['dh'][1:]:\n",
        "  if cnt<=hl[-1] or cnt-hl[-1]>9:\n",
        "    hl.append(hl[-1])\n",
        "  else:\n",
        "    hl.append(cnt)\n",
        "train_timing['dh2']=hl\n",
        "train_timing['dy2']=train_timing['dh2']//24\n",
        "train_timing['hr2']=train_timing['dh2']%24\n",
        "train_timing['Date_time5']=pd.to_datetime(\n",
        "    ('05-'+\n",
        "    (train_timing['dy2']).astype('int').astype('str')+\n",
        "    '-2022 '+\n",
        "    (train_timing['hr2']+100).astype('int').astype('str').str[1:]+\n",
        "    train_timing['hms'].str[-6:])\n",
        "    ,errors='coerce')\n",
        "timing=pd.Series(train_timing.Timing)\n",
        "timing.index=train_timing['Date_time5']\n",
        "train3=pd.DataFrame({'indext':train.index})\n",
        "train3.index=pd.to_datetime(train['date'],errors='coerce')\n",
        "train3['Timing']=timing[~timing.index.duplicated()]\n",
        "timing.index=timing.index+(timing.index[2]-timing.index[1])/2\n",
        "train3['Timing+1']=timing[~timing.index.duplicated()]\n",
        "timing.index=timing.index-(timing.index[2]-timing.index[1])\n",
        "train3['Timing-1']=timing[~timing.index.duplicated()]\n",
        "train3['cnt']=(1-train3['Timing+1'].isna())+(1-train3['Timing-1'].isna())\n",
        "train3['timing_1']=(train3['Timing+1'].fillna(0)+train3['Timing-1'].fillna(0))/train3['cnt']\n",
        "train3.loc[train3['Timing'].isna(),'Timing']=train3.loc[train3['Timing'].isna(),'timing_1']\n",
        "train3=train.merge(train3,how='left',on='indext')\n",
        "train3['File'].loc[:1269]='train1.avi'\n",
        "train3['File'].loc[1269:]='train2.avi'\n"
      ],
      "metadata": {
        "id": "hr_0IK4ecbP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#тайминг в тесте\n",
        "test_timing['hms1']=test_timing['Date_time'].str.extract(r\"([0-2]\\d:[0-5]\\d:[0-5]\\d)\")\n",
        "test_timing['hms2']=test_timing.loc[test_timing['hms1'].isna(),'Date_time'].str.extract(r\"(\\d:[0-5]\\d:[0-5]\\d)\")\n",
        "test_timing['hms']=(test_timing['hms1'].fillna('')+test_timing['hms2'].fillna(''))\n",
        "test_timing['d']=test_timing['Date_time'].str.extract(r\"(-2\\d-)\",expand=False).str.strip('-')\n",
        "test_timing['d']=pd.to_numeric(test_timing['d'],errors='coerce')\n",
        "dl=[test_timing['d'][0]]\n",
        "for cnt in test_timing['d'][1:]:\n",
        "  if cnt-dl[-1]==1:\n",
        "    dl.append(cnt)\n",
        "  else:\n",
        "    dl.append(dl[-1])\n",
        "test_timing['dy']=dl\n",
        "test_timing['h']=test_timing['hms'].str.split(':').str[0]\n",
        "test_timing['h']=pd.to_numeric(test_timing['h'],errors='coerce')\n",
        "hl=[test_timing['h'][0],test_timing['h'][0]]\n",
        "for cnt in zip(test_timing['h'][1:],test_timing['h'][1:].index):\n",
        "  print(cnt[0],hl[-1],min(cnt[0],hl[-1]+1))\n",
        "  if (cnt[0]>hl[-1] and cnt[0]-hl[-1]!=6) or hl[-1]-cnt[0]>9:\n",
        "    hl.append(min(cnt[0],hl[-2]+1))\n",
        "  else:\n",
        "    hl.append(hl[-1])\n",
        "test_timing['hr']=hl[1:]\n",
        "test_timing['Date_time4']=pd.to_datetime(\n",
        "    ('05-'+\n",
        "    (test_timing['dy']).astype('int').astype('str')+\n",
        "    '-2022 '+\n",
        "    (test_timing['hr']+100).astype('int').astype('str').str[1:]+\n",
        "    test_timing['hms'].str[-6:])\n",
        "    ,errors='coerce')\n",
        "timing=pd.Series(test_timing.index)\n",
        "timing.index=test_timing['Date_time4']\n",
        "test2=pd.DataFrame({'indext':test.index})\n",
        "test2.index=pd.to_datetime(test['date'],errors='coerce')\n",
        "timing = timing[~timing.index.duplicated()]\n",
        "test2['timing']=timing\n",
        "timing.index=timing.index+(timing.index[2]-timing.index[1])/2\n",
        "test2['timing+1']=timing\n",
        "timing.index=timing.index-(timing.index[2]-timing.index[1])\n",
        "test2['timing-1']=timing\n",
        "test2['cnt']=(1-test2['timing+1'].isna())+(1-test2['timing-1'].isna())\n",
        "test2['timing_1']=(test2['timing+1'].fillna(0)+test2['timing-1'].fillna(0))/test2['cnt']\n",
        "test2.loc[test2['timing'].isna(),'timing']=test2.loc[test2['timing'].isna(),'timing_1']\n"
      ],
      "metadata": {
        "id": "saSx2k_YcbC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#обучающий пул кадров\n",
        "train3['fname']=''\n",
        "for cnt1 in ['train1.avi','train2.avi']:\n",
        "  vs = cv2.VideoCapture('/content/train/'+cnt1)\n",
        "  for cnt2 in train3[train3['File']==cnt1][train3[train3['File']==cnt1]['Timing'].isna()==False].index:\n",
        "    vs.set(cv2.CAP_PROP_POS_MSEC,int(train3['Timing'][cnt2]*1000))\n",
        "    ret, frame = vs.read()\n",
        "    plt.imsave('/content/train3/'+str(cnt2)+'.png',frame)\n",
        "    train3.loc[cnt2,['fname']]='/content/train3/'+str(cnt2)+'.png'"
      ],
      "metadata": {
        "id": "vJ4jK33-bbRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#тестовый пул кадров\n",
        "test2['fname']=''\n",
        "vs = cv2.VideoCapture('/content/test/test.avi')\n",
        "for cnt2 in test2.index[test2.timing.isna()==False]:\n",
        "  vs.set(cv2.CAP_PROP_POS_MSEC,int(test2['timing'][cnt2]*1000))\n",
        "  ret, frame = vs.read()\n",
        "  plt.imsave('/content/test2/'+str(cnt2)+'.png',frame)\n",
        "  test2.loc[cnt2,['fname']]='/content/test2/'+str(cnt2)+'.png'"
      ],
      "metadata": {
        "id": "DBYlZXqcbbM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train2['employee']=train2['employee'].astype('str')\n",
        "train2['action']=train2['action'].astype('str')\n",
        "train2['date']=train2['date'].astype('str').str.replace('2027','2022')\n",
        "train2['date']=pd.to_datetime(train2['date'],errors='coerce')\n",
        "train2['1']=train2['date'].dt.dayofweek\n",
        "train2['2']=train2['date'].dt.hour\n",
        "train2['3']=train2['date'].dt.minute\n",
        "train2['4']=train2['date'].dt.second"
      ],
      "metadata": {
        "id": "v7RKPxpIjgag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2['1']=test2['date'].dt.dayofweek\n",
        "test2['2']=test2['date'].dt.hour\n",
        "test2['3']=test2['date'].dt.minute\n",
        "test2['4']=test2['date'].dt.second"
      ],
      "metadata": {
        "id": "87fGv5oqvJIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#обучение нейронных сетей\n",
        "traingen = tf.keras.preprocessing.image.ImageDataGenerator(brightness_range=(0.9,1.1),channel_shift_range=0.1,\n",
        "    preprocessing_function=tf.keras.applications.xception.preprocess_input)\n",
        "traingen2 = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=(0.9,1.1),\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.0,\n",
        "    preprocessing_function=tf.keras.applications.xception.preprocess_input)\n",
        "valgen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.xception.preprocess_input)"
      ],
      "metadata": {
        "id": "k9FQDn9ekd6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traingt=traingen.flow_from_dataframe(train2[train2['fname'].isna()==False], x_col='fname', y_col='action', \n",
        "                                    weight_col=None, target_size=(299, 299), \n",
        "                                    classes=list(train2.action.unique()),\n",
        "                                    class_mode='categorical', batch_size=64)\n",
        "traing2t=traingen2.flow_from_dataframe(train2[train2['fname'].isna()==False], x_col='fname', y_col='employee', \n",
        "                                    weight_col=None, target_size=(299, 299), \n",
        "                                    classes=list(train2.employee.unique()),\n",
        "                                    class_mode='categorical', batch_size=64)"
      ],
      "metadata": {
        "id": "8kvkpLO6klNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=tf.keras.Input(shape=(299, 299, 3))\n",
        "\n",
        "lay1=tf.keras.layers.Conv2D(8,(3,3),activation='relu')(inp)\n",
        "lay2=tf.keras.layers.MaxPooling2D((5,5))(lay1)\n",
        "lay3=tf.keras.layers.Conv2D(32,(3,3),activation='relu')(lay2)\n",
        "lay4=tf.keras.layers.MaxPooling2D((2,2))(lay3)\n",
        "lay5=tf.keras.layers.Flatten()(lay4)\n",
        "lay6=tf.keras.layers.Dense(18,activation='softmax')(lay5)\n",
        "\n",
        "\n",
        "model1 = tf.keras.Model(inputs=inp,outputs=lay6)\n",
        "optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
        "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy',tf.keras.metrics.Recall()])\n",
        "\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "lQxmoG94kyby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model1.fit(traingt,epochs=30)"
      ],
      "metadata": {
        "id": "T0BpGUSFk3GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=tf.keras.Input(shape=(299, 299, 3))\n",
        "\n",
        "lay1=tf.keras.layers.Conv2D(16,(3,3),activation='relu')(inp)\n",
        "lay2=tf.keras.layers.MaxPooling2D((2,2))(lay1)\n",
        "lay3=tf.keras.layers.Conv2D(32,(3,3),activation='relu')(lay2)\n",
        "lay4=tf.keras.layers.MaxPooling2D((2,2))(lay3)\n",
        "lay5=tf.keras.layers.Conv2D(64,(3,3),activation='relu')(lay4)\n",
        "lay6=tf.keras.layers.MaxPooling2D((2,2))(lay5)\n",
        "lay7=tf.keras.layers.Conv2D(128,(3,3),activation='relu')(lay6)\n",
        "lay8=tf.keras.layers.MaxPooling2D((2,2))(lay7)\n",
        "lay9=tf.keras.layers.Conv2D(256,(3,3),activation='relu')(lay8)\n",
        "lay10=tf.keras.layers.GlobalMaxPooling2D()(lay9)\n",
        "lay11=tf.keras.layers.Dense(64,activation='relu')(lay10)\n",
        "lay12=tf.keras.layers.Dense(11,activation='softmax')(lay11)\n",
        "\n",
        "\n",
        "model2 = tf.keras.Model(inputs=inp,outputs=lay12)\n",
        "optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
        "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy',tf.keras.metrics.Recall()])\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "BzJKXul0uk-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model2.fit(traing2t,epochs=30)"
      ],
      "metadata": {
        "id": "9K33QdMDurU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data=valgen.flow_from_dataframe(train2[train2['fname'].isna()==False], x_col='fname', y_col='employee', \n",
        "                                    weight_col=None, target_size=(299, 299), \n",
        "                                    class_mode='categorical', batch_size=128,shuffle=False)\n",
        "X_datat=valgen.flow_from_dataframe(test2[test2['fname'].isna()==False], x_col='fname', y_col=None, \n",
        "                                    weight_col=None, target_size=(299, 299), \n",
        "                                    class_mode=None, batch_size=128,shuffle=False)"
      ],
      "metadata": {
        "id": "mWS-iutek9vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1=model1.predict(X_data)\n",
        "X1_1=np.zeros((len(train2),X1.shape[1]))\n",
        "X1_1[train2['fname'].isna()==False]=X1"
      ],
      "metadata": {
        "id": "t7SlGOiFlVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1t=model1.predict(X_datat)\n",
        "X1t_1=np.zeros((len(test2),X1_1.shape[1]))\n",
        "X1t_1[test2['fname'].isna()==False]=X1t"
      ],
      "metadata": {
        "id": "Ux8_2E4ylSHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2=model2.predict(X_data)\n",
        "X2_1=np.zeros((len(train2),X2.shape[1]))\n",
        "X2_1[train2['fname'].isna()==False]=X2"
      ],
      "metadata": {
        "id": "Z6JXSkJAuG-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2t=model2.predict(X_datat)\n",
        "X2t_1=np.zeros((len(test2),X2_1.shape[1]))\n",
        "X2t_1[test2['fname'].isna()==False]=X2t"
      ],
      "metadata": {
        "id": "Aj2-TJaFu5EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#конечный прогноз\n",
        "pipe=iPipeline([\n",
        "                ('im',SMOTETomek()),\n",
        "                ('cls',RandomForestClassifier(200))])\n",
        "test2['employee']=pipe.fit(np.hstack((X1_1,X2_1,train2[['1','2','3','4']].fillna(-1).values)),\n",
        "                    train2['employee']).predict(np.hstack((X1t_1,X2t_1,test2[['1','2','3','4']].fillna(-1).values)))\n",
        "test2['action']=pipe.fit(np.hstack((X1_1,X2_1,train2[['1','2','3','4']].fillna(-1).values))[train2['action']!='10.0'],\n",
        "                    train2['action'][train2['action']!='10.0']).predict(np.hstack((X1t_1,X2t_1,test2[['1','2','3','4']].fillna(-1).values)))"
      ],
      "metadata": {
        "id": "1VavmCVzu5Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2['employee']=test2['employee'].astype('int')\n",
        "test2['action']=test2['action'].astype('float').astype('int')"
      ],
      "metadata": {
        "id": "PHt7K9dVu48z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=test2[['id', 'employee', 'action']]\n",
        "answer.to_csv('submit.csv',index=False)"
      ],
      "metadata": {
        "id": "AhFSVNuIvwIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}